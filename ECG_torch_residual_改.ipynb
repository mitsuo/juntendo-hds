{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitsuo/juntendo-hds/blob/main/ECG_torch_residual_%E6%94%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ライブラリ読み込み\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os, glob, pickle, time, gc, copy, sys\n",
        "import warnings\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 100) # 表示できる表の列数"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "xdhVBNMyStXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3A1_BmDc-b"
      },
      "source": [
        "# 心電図データのダウンロード\n",
        "今回は 全国医療AIコンテスト 2021 (https://www.kaggle.com/competitions/ai-medical-contest-2021/ ) の心電図データをダウンロードします。このデータは心電図から心筋梗塞かどうかを判定するタスクのためのものです。\n",
        "\n",
        "まずデータをダウンロードしましょう。下のセルでは `ai-medical-contest-2021` というディレクトリをデフォルトのディレクトリ `/content` の下に作成し、そこに関連データをダウンロードしています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTN_9KKyMGIJ"
      },
      "source": [
        "# ! を先頭につけると一時的に適応される。例えばワーキングディレクトリの移動をしてもその後のコマンドには適応されない。\n",
        "# ai-medical-contest-2021 ディレクトリを作成します。\n",
        "!rm -rf /content/ai-medical-contest-2021\n",
        "!mkdir /content/ai-medical-contest-2021\n",
        "\n",
        "# % を先頭につけると永続的に適応される。ワーキングディレクトリの移動をしてもその後も適応される。\n",
        "# ai-medical-contest-2021 ディレクトリに移動します。\n",
        "%cd /content/ai-medical-contest-2021\n",
        "!pwd\n",
        "!ls\n",
        "\n",
        "# 心電図データのダウンロード\n",
        "!wget http://mitsuo.nishizawa.com/juntendo/ai-medical-contest-2021.zip\n",
        "!unzip ai-medical-contest-2021.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainファイルを読み込む\n",
        "df_train = pd.read_csv(\"/content/ai-medical-contest-2021/train.csv\")\n",
        "print(\"df_train.shape\", df_train.shape) # シェイプ = (行数, 列数)を表示する\n",
        "\n",
        "# testファイルを読み込む\n",
        "df_test = pd.read_csv(\"/content/ai-medical-contest-2021//test.csv\")\n",
        "print(\"df_test.shape\", df_test.shape) # シェイプ = (行数, 列数)を表示する\n",
        "\n",
        "# submissionファイルを読み込む\n",
        "df_sub = pd.read_csv(\"/content/ai-medical-contest-2021//sample_submission.csv\")\n",
        "print(\"df_sub.shape\", df_sub.shape) # シェイプ = (行数, 列数)を表示する\n",
        "\n",
        "# ECGデータのpathの列を追加.\n",
        "df_train['path'] = df_train['Id'].apply(lambda x: \"/content/ai-medical-contest-2021/ecg/{}.npy\".format(x))\n",
        "df_test['path'] = df_test['Id'].apply(lambda x: \"/content/ai-medical-contest-2021/ecg/{}.npy\".format(x))\n",
        "print(df_train['path'][0]) # path列の0行目を表示\n",
        "\n",
        "# trainとtestを連結する\n",
        "df_traintest = pd.concat([df_train, df_test]).reset_index(drop=True) # reset_index: 行のindexをリセットする\n",
        "print(df_traintest.shape)\n",
        "\n",
        "col_target = 'target' # ターゲットの列\n",
        "col_index = 'Id' # idの列\n",
        "print(\"rate of positive: {:.6f}\".format(df_train[col_target].mean())) # targetが1である割合\n",
        "\n",
        "# 各列の基本情報を表示\n",
        "# 解析対象はtrain+test\n",
        "# 列名, 型, nanの数, uniqueな値の数, 実際の値の一部, を表示する\n",
        "df_tmp = df_traintest  # 解析するDataFrameを指定\n",
        "for i, col in enumerate(df_tmp.columns): # 各列(column)について\n",
        "    col_name = col + \" \" * (22 - len(col)) # カラム名, 見た目上の整形のためにスペースを加える\n",
        "    type_name = \"{}\".format(df_tmp[col].dtype) # 型名\n",
        "    type_name = type_name + \" \" * (8 - len(type_name)) # 見た目上の整形のためにスペースを加える\n",
        "    num_unique = len(df_tmp[col].unique()) # ユニークな値の数\n",
        "    num_nan = pd.isna(df_tmp[col]).sum() # nanの数\n",
        "    col_head = \"{}\".format(df_tmp[col].unique()[:5].tolist())[:40] # 実際の値の一部\n",
        "    print(\"{:4d}: {} dtype: {} unique: {:8d}, nan: {:6d}, 実際の値: {}\".format(\n",
        "        i, col_name, type_name, len(df_tmp[col].unique()), num_nan, col_head)) # 表示する\n",
        "\n",
        "# カテゴリ変数をラベルエンコーディングする (数値に置き換える).\n",
        "df_traintest['sex'] = df_traintest['sex'].replace('female', 0) # femaleに0を代入\n",
        "df_traintest['sex'] = df_traintest['sex'].replace('male', 1) # maleに1を代入\n",
        "df_traintest['sex'] = df_traintest['sex'].astype(int) # 型を整数に変換\n",
        "\n",
        "df_traintest['label_type'] = df_traintest['label_type'].replace('human', 0) # humanに0を代入\n",
        "df_traintest['label_type'] = df_traintest['label_type'].replace('auto', 1) # autoに1を代入\n",
        "df_traintest['label_type'] = df_traintest['label_type'].astype(int) # 型を整数に変換\n",
        "\n",
        "# train と test を再度切り分ける\n",
        "df_train = df_traintest.iloc[:len(df_train)]\n",
        "df_test = df_traintest.iloc[len(df_train):].reset_index(drop=True)\n",
        "\n",
        "# 全てのECGデータを読み込む\n",
        "ecg_train = np.zeros([len(df_train), 800, 12], np.float32) # trainの心電図データの代入先. shape=(データ数, 時間方向, 12誘導)\n",
        "for i in range(len(df_train)): # 全てのtrain dataについて\n",
        "    path_tmp = df_train['path'][i] # i行目の心電図データのpath\n",
        "    ecg_tmp = np.load(path_tmp) # i行目の心電図データ\n",
        "    ecg_train[i] = ecg_tmp # 読み込んだ心電図データをecg_trainのi行目に代入\n",
        "\n",
        "ecg_test = np.zeros([len(df_test), 800, 12], np.float32) # testの心電図データの代入先. shape=(データ数, 時間方向, 12誘導)\n",
        "for i in range(len(df_test)): # 全てのtest dataについて\n",
        "    path_tmp = df_test['path'][i] # i行目の心電図データのpath\n",
        "    ecg_tmp = np.load(path_tmp) # i行目の心電図データ\n",
        "    ecg_test[i] = ecg_tmp # 読み込んだ心電図データをecg_trainのi行目に代入\n",
        "print(\"ecg_train.shape: {}\".format(ecg_train.shape))\n",
        "print(\"ecg_test.shape: {}\".format(ecg_test.shape))\n",
        "\n",
        "# target情報をnumpy形式に変換\n",
        "target_train = df_train[col_target].values.astype(int) # pandas.Seriesからnp.ndarrayへ変換\n",
        "print(\"target_train.shape: {}\".format(target_train.shape))"
      ],
      "metadata": {
        "trusted": true,
        "id": "RmM9_PzZStXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# クロスバリデーションを行うためにデータを5分割する\n",
        "# 4つを学習に用い、1つを検証に要する。これを5回繰り返す。\n",
        "folds = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(\n",
        "    np.arange(len(df_train)),\n",
        "    y=df_train[col_target]) # 各foldターゲットのラベルの分布がそろうようにする = stratified K fold\n",
        ")\n",
        "\n",
        "# fold 0の学習データと検証データの分割\n",
        "fold = 0 # fold 0 についての学習を行う\n",
        "\n",
        "# このfoldにおける学習データと検証データの切り分け\n",
        "X_train = ecg_train[folds[fold][0]] # 学習データの入力データを抽出\n",
        "y_train = target_train[folds[fold][0]] # 学習データの正解データを抽出\n",
        "X_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\n",
        "y_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\n",
        "print(\"X_train.shape: {}, X_valid.shape: {}\".format(X_train.shape, X_valid.shape))\n",
        "print(\"y_train.shape: {}, y_valid.shape: {}\".format(y_train.shape, y_valid.shape))"
      ],
      "metadata": {
        "trusted": true,
        "id": "6GbiZDuOStXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pytorch"
      ],
      "metadata": {
        "id": "B_MenRtUStXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "PhIvQ_vYStXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 700\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, label):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "        self.len = data.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        out_data = self.data\n",
        "        out_data = out_data[index]\n",
        "\n",
        "        start_idx = np.random.randint(0,800-input_size-1)\n",
        "        out_data = out_data[start_idx:start_idx+input_size:,:]\n",
        "        out_label = self.label[index]\n",
        "\n",
        "        return out_data, out_label"
      ],
      "metadata": {
        "trusted": true,
        "id": "LM_KaKkuStXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net1D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(12, 64, kernel_size=7, stride=1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(128,256,kernel_size=3, stride=2)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(256,1)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "#         s1, s2, s3 = x.shape\n",
        "#         x = x.reshape(s1, s3, s2)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "trusted": true,
        "id": "MBqi0tVjStXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/eddymina/ECG_Classification_Pytorch/blob/master/ECG_notebook.ipynb\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"\"\"\\nA 1D CNN is very effective when you expect to derive interesting features from shorter\n",
        "(fixed-length) segments of the overall data set and where the location of the feature\n",
        "within the segment is not of high relevance.\\n\"\"\")\n",
        "\n",
        "class Anomaly_Classifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Anomaly_Classifier, self).__init__()\n",
        "\n",
        "        self.conv= nn.Conv1d(in_channels=12, out_channels=32, kernel_size=5,stride=1)\n",
        "\n",
        "        self.conv_pad = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5,stride=1,padding=2)\n",
        "        self.drop_50 = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=5,stride=2)\n",
        "\n",
        "#         self.dense1 = nn.Linear(32 * 8, 32)\n",
        "        self.dense1 = nn.Linear(1280, 32)\n",
        "        self.dense2 = nn.Linear(32, 32)\n",
        "\n",
        "        self.dense_final = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        residual= self.conv(x)\n",
        "\n",
        "        #block1\n",
        "        x = F.relu(self.conv_pad(residual))\n",
        "        x = self.conv_pad(x)\n",
        "        x+= residual\n",
        "        x = F.relu(x)\n",
        "        residual = self.maxpool(x) #[512 32 90]\n",
        "\n",
        "        #block2\n",
        "        x=F.relu(self.conv_pad(residual))\n",
        "        x=self.conv_pad(x)\n",
        "        x+=residual\n",
        "        x= F.relu(x)\n",
        "        residual = self.maxpool(x) #[512 32 43]\n",
        "\n",
        "\n",
        "        #block3\n",
        "        x=F.relu(self.conv_pad(residual))\n",
        "        x=self.conv_pad(x)\n",
        "        x+=residual\n",
        "        x= F.relu(x)\n",
        "        residual = self.maxpool(x) #[512 32 20]\n",
        "\n",
        "\n",
        "        #block4\n",
        "        x=F.relu(self.conv_pad(residual))\n",
        "        x=self.conv_pad(x)\n",
        "        x+=residual\n",
        "        x= F.relu(x)\n",
        "        x= self.maxpool(x) #[512 32 8]\n",
        "\n",
        "        s1, s2, s3 = x.shape\n",
        "\n",
        "        #MLP\n",
        "        x = x.view(-1, s2 * s3) #Reshape (current_dim, 32*2)\n",
        "#         print(x.shape)\n",
        "\n",
        "        x = F.relu(self.dense1(x))\n",
        "        #x = self.drop_60(x)\n",
        "        x= self.dense2(x)\n",
        "        x = self.dense_final(x)\n",
        "        x = x.view(-1)\n",
        "        return x"
      ],
      "metadata": {
        "trusted": true,
        "id": "4KJzkIQqStXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net1D()\n",
        "print(model)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mvCgER-UStXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = 0\n",
        "n_splits = 5\n",
        "optimizer_name = 'Adam'\n",
        "lr = 0.001\n",
        "EPOCHS=30\n",
        "\n",
        "list_weights = []\n",
        "best_preds_list = []\n",
        "valid_label_list = []\n",
        "for fold in range(n_splits):\n",
        "    X_train = ecg_train[folds[fold][0]] # 学習データの入力データを抽出\n",
        "    y_train = target_train[folds[fold][0]] # 学習データの正解データを抽出\n",
        "    X_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\n",
        "    y_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\n",
        "\n",
        "    X_train = torch.FloatTensor(X_train).to(device)\n",
        "    y_train = torch.FloatTensor(y_train).to(device)\n",
        "    X_valid = torch.FloatTensor(X_valid).to(device)\n",
        "    y_valid = torch.FloatTensor(y_valid).to(device)\n",
        "\n",
        "    dataset = MyDataset(X_train, y_train)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    dataset_val = MyDataset(X_valid, y_valid)\n",
        "    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=64, shuffle=False)\n",
        "\n",
        "#     model = Net1D().cuda()\n",
        "    model = Anomaly_Classifier(num_classes= 1).to(device)\n",
        "    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    best_auc = 0\n",
        "    for e in range(EPOCHS):\n",
        "        avg_loss = 0\n",
        "        model.train()\n",
        "        for i, (data, y_target) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(data)\n",
        "            loss = criterion(y_pred, y_target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss.item() / len(dataloader)\n",
        "\n",
        "        model.eval()\n",
        "        avg_val_loss = 0.\n",
        "        valid_labels = []\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for i, (data, y_target) in enumerate(dataloader_val):\n",
        "                y_pred = model(data)\n",
        "                valloss = criterion(y_pred, y_target)\n",
        "                avg_val_loss += valloss.item() / len(dataloader_val)\n",
        "                valid_labels.append(y_target.to('cpu').numpy())\n",
        "                preds.append(F.sigmoid(y_pred).cpu().numpy())\n",
        "        preds = np.concatenate(preds)\n",
        "        valid_labels = np.concatenate(valid_labels)\n",
        "        val_auc = roc_auc_score(valid_labels,preds[:])\n",
        "\n",
        "        if e % 1 == 0:\n",
        "            print('E {}: train loss: {} val loss: {} val AUC: {}'.format(\n",
        "                e, avg_loss, avg_val_loss, val_auc))\n",
        "\n",
        "        if best_auc < val_auc:\n",
        "            best_auc = val_auc\n",
        "            best_preds = preds\n",
        "            print(f'  Epoch {e} - Save Best AUC: {best_auc:.4f}')\n",
        "            best_weight = model.state_dict()\n",
        "\n",
        "    list_weights.append(best_weight)\n",
        "    best_preds_list.append(best_preds)\n",
        "    valid_label_list.append(valid_labels)\n",
        "\n",
        "## calc oof\n",
        "best_preds_list = np.concatenate(best_preds_list)\n",
        "valid_label_list = np.concatenate(valid_label_list)\n",
        "oof_auc = roc_auc_score(valid_labels,preds[:])\n",
        "print(f\"OOF_AUC{oof_auc}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "KbwK2aI-StXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_test = np.zeros([n_splits, len(df_test)], np.float32) # 予測結果の代入先\n",
        "for fold, w in tqdm(enumerate(list_weights)):\n",
        "#     model = Anomaly_Classifier(input_size=12, num_classes= 1).to(device)\n",
        "    list_test = []\n",
        "\n",
        "    X_test = torch.FloatTensor(ecg_test).to(device)\n",
        "\n",
        "    dataset_test = MyDataset(X_test, np.zeros(X_test.shape[0]))\n",
        "    dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=64, shuffle=False)\n",
        "\n",
        "#     model = Net1D().to(device)\n",
        "    model = Anomaly_Classifier(num_classes= 1).to(device)\n",
        "    model.load_state_dict(w)\n",
        "    model.eval()\n",
        "    l_p = []\n",
        "    with torch.no_grad():\n",
        "        for i, (data, y_target) in enumerate(dataloader_test):\n",
        "            y_p = model(data)\n",
        "            y_p = F.sigmoid(y_p).cpu().numpy()\n",
        "            l_p.append(y_p)\n",
        "    y_pred = np.concatenate(l_p)\n",
        "    preds_test[fold] = y_pred"
      ],
      "metadata": {
        "trusted": true,
        "id": "b7XTT4OMStXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "trusted": true,
        "id": "THoXqz2jStXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(preds_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q0qzS2_GStXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### submitファイルを作成\n",
        "preds_test_mean = preds_test.mean(axis=0) # 各foldのmodelの予測の平均値を最終的な予測結果として採用する\n",
        "print(\"preds_test_mean.shape: {}\".format(preds_test_mean.shape))\n",
        "df_sub[col_target] = preds_test.mean(axis=0) # 推定結果を代入\n",
        "# df_sub[col_target] = preds_test[4]\n",
        "df_sub.to_csv(\"submission.torch.csv\", index=None) # submitファイルを保存\n",
        "df_sub.head() # 最初の5行を表示"
      ],
      "metadata": {
        "trusted": true,
        "id": "vKUG47vKStXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}