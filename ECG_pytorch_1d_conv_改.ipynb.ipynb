{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitsuo/juntendo-hds/blob/main/ECG_pytorch_1d_conv_%E6%94%B9.ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "qIBbwEeqHxiS"
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import random\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "from scipy.signal import find_peaks, resample\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print('Packages Loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3A1_BmDc-b"
      },
      "source": [
        "# 心電図データのダウンロード\n",
        "今回は 全国医療AIコンテスト 2021 (https://www.kaggle.com/competitions/ai-medical-contest-2021/ ) の心電図データをダウンロードします。このデータは心電図から心筋梗塞かどうかを判定するタスクのためのものです。\n",
        "\n",
        "まずデータをダウンロードしましょう。下のセルでは `ai-medical-contest-2021` というディレクトリをデフォルトのディレクトリ `/content` の下に作成し、そこに関連データをダウンロードしています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTN_9KKyMGIJ"
      },
      "source": [
        "# ! を先頭につけると一時的に適応される。例えばワーキングディレクトリの移動をしてもその後のコマンドには適応されない。\n",
        "# ai-medical-contest-2021 ディレクトリを作成します。\n",
        "!rm -rf /content/ai-medical-contest-2021\n",
        "!mkdir /content/ai-medical-contest-2021\n",
        "\n",
        "# % を先頭につけると永続的に適応される。ワーキングディレクトリの移動をしてもその後も適応される。\n",
        "# ai-medical-contest-2021 ディレクトリに移動します。\n",
        "%cd /content/ai-medical-contest-2021\n",
        "!pwd\n",
        "!ls\n",
        "\n",
        "# 心電図データのダウンロード\n",
        "!wget http://mitsuo.nishizawa.com/juntendo/ai-medical-contest-2021.zip\n",
        "!unzip ai-medical-contest-2021.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "cj5h80RxHxiT"
      },
      "cell_type": "code",
      "source": [
        "data_dir = '/content/ai-medical-contest-2021'\n",
        "train_path = f'{data_dir}/train.csv'\n",
        "test_path = f'{data_dir}/test.csv'\n",
        "\n",
        "col_target = 'target'\n",
        "col_index = 'Id'\n",
        "col_features = ['age', 'sex', 'label_type']\n",
        "\n",
        "SEED = 42\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "RsZo9LGyHxiU"
      },
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(train_path)\n",
        "print(\"df_train.shape\", df_train.shape)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "RlJCh0SzHxiU"
      },
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(test_path)\n",
        "print(\"df_test.shape\", df_test.shape)\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "zuZ0sWAkHxiV"
      },
      "cell_type": "code",
      "source": [
        "df_traintest = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
        "df_traintest['path'] = df_traintest['Id'].apply(lambda x: f\"{data_dir}/ecg/{x}.npy\")\n",
        "print(df_traintest['path'][0])\n",
        "df_traintest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DzPkHGqmHxiV"
      },
      "cell_type": "markdown",
      "source": [
        "# 前処理"
      ]
    },
    {
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "jwKkKuIdHxiW"
      },
      "cell_type": "code",
      "source": [
        "df_traintest['sex'] = df_traintest['sex'].replace('female', 0)\n",
        "df_traintest['sex'] = df_traintest['sex'].replace('male', 1)\n",
        "df_traintest['sex'] = df_traintest['sex'].astype(int)\n",
        "\n",
        "df_traintest['label_type'] = df_traintest['label_type'].replace('human', 0)\n",
        "df_traintest['label_type'] = df_traintest['label_type'].replace('auto', 1)\n",
        "df_traintest['label_type'] = df_traintest['label_type'].astype(int)\n",
        "df_traintest.head()\n",
        "df_traintest['sex'] = df_traintest['sex'].replace('female', 0)\n",
        "df_traintest['sex'] = df_traintest['sex'].replace('male', 1)\n",
        "df_traintest['sex'] = df_traintest['sex'].astype(int)\n",
        "\n",
        "df_traintest['label_type'] = df_traintest['label_type'].replace('human', 0)\n",
        "df_traintest['label_type'] = df_traintest['label_type'].replace('auto', 1)\n",
        "df_traintest['label_type'] = df_traintest['label_type'].astype(int)\n",
        "\n",
        "df_traintest['age'] = (df_traintest['age'] - df_traintest['age'].min()) / (df_traintest['age'].max() - df_traintest['age'].min())\n",
        "df_traintest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "Tckj34ACHxiX"
      },
      "cell_type": "code",
      "source": [
        "df_train = df_traintest.iloc[:len(df_train)]\n",
        "df_test = df_traintest.iloc[len(df_train):].reset_index(drop=True)\n",
        "print(df_train.shape, df_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "9JruOWmRHxiX"
      },
      "cell_type": "code",
      "source": [
        "ecg_train = np.zeros([len(df_train), 800, 12], np.float32)\n",
        "for i in range(len(df_train)):\n",
        "    path_tmp = df_train['path'][i]\n",
        "    ecg_tmp = np.load(path_tmp)\n",
        "    ecg_train[i] = ecg_tmp\n",
        "\n",
        "ecg_test = np.zeros([len(df_test), 800, 12], np.float32)\n",
        "for i in range(len(df_test)):\n",
        "    path_tmp = df_test['path'][i]\n",
        "    ecg_tmp = np.load(path_tmp)\n",
        "    ecg_test[i] = ecg_tmp\n",
        "\n",
        "ecg_train = ecg_train.transpose(0, 2, 1)\n",
        "ecg_test = ecg_test.transpose(0, 2, 1)\n",
        "print(\"ecg_train.shape: {}\".format(ecg_train.shape))\n",
        "print(\"ecg_test.shape: {}\".format(ecg_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "dqOF8xcPHxiY"
      },
      "cell_type": "code",
      "source": [
        "target_train = df_train[col_target].values.astype(np.float32)\n",
        "print(\"target_train.shape: {}\".format(target_train.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Kg-UlljHxiY"
      },
      "cell_type": "markdown",
      "source": [
        "# DA"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XFA2tLXNHxiZ"
      },
      "cell_type": "code",
      "source": [
        "def stretch(x, l):\n",
        "    y = resample(x, l)\n",
        "    if l < 800:\n",
        "        y_ = np.zeros(shape=(800, ))\n",
        "        y_[:l] = y\n",
        "    else:\n",
        "        y_ = y[:800]\n",
        "    return y_\n",
        "\n",
        "def amplify(x, alpha):\n",
        "    factor = -alpha*x + (1+alpha)\n",
        "    return x*factor\n",
        "\n",
        "def stretch_twelve(ecg):\n",
        "    l = int(800 * (1 + (random.random()-0.5)/3))\n",
        "    *y_, = map(stretch, ecg, [l] * 12)\n",
        "    return np.array(y_, dtype=np.float32)\n",
        "\n",
        "def amplify_twelve(ecg):\n",
        "    alpha = (random.random()-0.5)\n",
        "    *y_, = map(amplify, ecg, [alpha] * 12)\n",
        "    return np.array(y_, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kkZPVMUEHxiZ"
      },
      "cell_type": "code",
      "source": [
        "def get_train_transforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Lambda(amplify_twelve),\n",
        "        transforms.Lambda(stretch_twelve),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "def get_valid_transforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFEIyHzmHxia"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-xdbUTx_Hxia"
      },
      "cell_type": "code",
      "source": [
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X, X_add=None, y=None, train=True, transforms=None):\n",
        "        super().__init__()\n",
        "        self.X = X\n",
        "        self.X_add = X_add\n",
        "        self.y = y\n",
        "        self.train = train\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X_trans = self.transforms(self.X[index])[0]\n",
        "        if self.X_add is not None:\n",
        "            X_add = torch.tensor(self.X_add[index], dtype=torch.float)\n",
        "        else:\n",
        "            X_add = None\n",
        "        if self.train == True:\n",
        "            return X_trans, X_add, torch.tensor(self.y[index], dtype=torch.float)\n",
        "        else:\n",
        "            return X_trans, X_add"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTSAAOvrHxia"
      },
      "cell_type": "markdown",
      "source": [
        "# model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Qf-rr3clHxib"
      },
      "cell_type": "code",
      "source": [
        "class Anomaly_Classifier(nn.Module):\n",
        "    def __init__(self, input_size,num_classes):\n",
        "        super(Anomaly_Classifier, self).__init__()\n",
        "\n",
        "        self.conv= nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=5,stride=1)\n",
        "\n",
        "        self.conv_pad = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5,stride=1,padding=2)\n",
        "        self.drop_50 = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=5,stride=2)\n",
        "\n",
        "        self.dense1 = nn.Linear(32 * 46, 32)\n",
        "        self.dense1_add = nn.Linear(32 * 46 + len(col_features), 32)\n",
        "        self.dense2 = nn.Linear(32, 32)\n",
        "\n",
        "        self.dense_final = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x, x_add=None):\n",
        "        residual= self.conv(x)\n",
        "\n",
        "        #block1\n",
        "        x = F.relu(self.conv_pad(residual))\n",
        "        x = self.conv_pad(x)\n",
        "        x+= residual\n",
        "        x = F.relu(x)\n",
        "        residual = self.maxpool(x)\n",
        "\n",
        "        #block2\n",
        "        x=F.relu(self.conv_pad(residual))\n",
        "        x=self.conv_pad(x)\n",
        "        x+=residual\n",
        "        x= F.relu(x)\n",
        "        residual = self.maxpool(x)\n",
        "\n",
        "        #block3\n",
        "        x=F.relu(self.conv_pad(residual))\n",
        "        x=self.conv_pad(x)\n",
        "        x+=residual\n",
        "        x= F.relu(x)\n",
        "        residual = self.maxpool(x)\n",
        "\n",
        "\n",
        "        #block4\n",
        "        x=F.relu(self.conv_pad(residual))\n",
        "        x=self.conv_pad(x)\n",
        "        x+=residual\n",
        "        x= F.relu(x)\n",
        "        x= self.maxpool(x)\n",
        "\n",
        "        #MLP\n",
        "        x = x.view(-1, 32 * 46)\n",
        "        if x_add is not None:\n",
        "            x = torch.cat([x, x_add], axis=1)\n",
        "            x = F.relu(self.dense1_add(x))\n",
        "        else:\n",
        "            x = F.relu(self.dense1(x))\n",
        "        x= self.dense2(x)\n",
        "        x = self.dense_final(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9EmvxKP5Hxib"
      },
      "cell_type": "markdown",
      "source": [
        "# train"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zywadFMqHxic"
      },
      "cell_type": "code",
      "source": [
        "def run_one_fold(train_X, train_y, valid_X, valid_y, num_fold, train_X_add=None, valid_X_add = None):\n",
        "    train_dataset = ECGDataset(train_X, train_X_add, train_y, transforms=get_train_transforms())\n",
        "    valid_dataset = ECGDataset(valid_X, valid_X_add, valid_y, transforms=get_valid_transforms())\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=DataLoaderConfig.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=DataLoaderConfig.num_workers,\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=DataLoaderConfig.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=DataLoaderConfig.num_workers,\n",
        "    )\n",
        "\n",
        "    fitter = Fitter(\n",
        "        model=net,\n",
        "        device=DEVICE,\n",
        "        criterion=TrainConfig.criterion,\n",
        "        n_epochs=TrainConfig.n_epochs,\n",
        "        lr=TrainConfig.lr,\n",
        "        sheduler=TrainConfig.scheduler,\n",
        "        scheduler_params=TrainConfig.scheduler_params\n",
        "    )\n",
        "    fitter.fit(train_loader, valid_loader, num_fold=num_fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "NWzEwI45Hxic"
      },
      "cell_type": "code",
      "source": [
        "class LossMeter:\n",
        "    def __init__(self):\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class AccMeter:\n",
        "    def __init__(self):\n",
        "        self.y_preds = []\n",
        "        self.y_trues = []\n",
        "\n",
        "    def update(self, y_true, y_pred):\n",
        "        self.y_preds += list(y_pred.sigmoid().cpu().numpy().ravel())\n",
        "        self.y_trues += list(y_true.cpu().numpy().ravel())\n",
        "\n",
        "    def auc(self):\n",
        "        if len(self.y_preds) == 0 or len(self.y_trues) == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return roc_auc_score(self.y_trues, self.y_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9VLMhZRvHxic"
      },
      "cell_type": "code",
      "source": [
        "class Fitter:\n",
        "    def __init__(\n",
        "        self, model, device, criterion, n_epochs,\n",
        "        lr, sheduler=None, scheduler_params=None\n",
        "    ):\n",
        "        self.epoch = 0\n",
        "        self.n_epochs = n_epochs\n",
        "        self.base_dir = './'\n",
        "        self.log_path = f'{self.base_dir}/log.txt'\n",
        "        self.best_summary_loss = np.inf\n",
        "\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "        if sheduler:\n",
        "            self.scheduler = sheduler(self.optimizer, **scheduler_params)\n",
        "\n",
        "        self.criterion = criterion().to(self.device)\n",
        "\n",
        "        self.log(f'Fitter prepared. Device is {self.device}')\n",
        "\n",
        "    def fit(self, train_loader, valid_loader, num_fold=0):\n",
        "        for e in range(self.n_epochs):\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            self.log(f'\\n{datetime.datetime.utcnow().isoformat()}\\nLR: {current_lr}')\n",
        "\n",
        "            t = int(time.time())\n",
        "            summary_loss, final_scores = self.train_one_epoch(train_loader)\n",
        "            self.log(\n",
        "                f'[RESULT]: Train. Epoch: {self.epoch}, ' + \\\n",
        "                f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
        "                f'final_score: {final_scores.auc():.5f}, ' + \\\n",
        "                f'time: {int(time.time()) - t} s'\n",
        "            )\n",
        "\n",
        "            t = int(time.time())\n",
        "            summary_loss, final_scores = self.validation(valid_loader)\n",
        "            self.log(\n",
        "                f'[RESULT]: Valid. Epoch: {self.epoch}, ' + \\\n",
        "                f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
        "                f'final_score: {final_scores.auc():.5f}, ' + \\\n",
        "                f'time: {int(time.time()) - t} s'\n",
        "            )\n",
        "\n",
        "            f_best = 0\n",
        "            if summary_loss.avg < self.best_summary_loss:\n",
        "                self.best_summary_loss = summary_loss.avg\n",
        "                f_best = 1\n",
        "\n",
        "\n",
        "            self.scheduler.step(metrics=summary_loss.avg)\n",
        "\n",
        "            self.save(f'{self.base_dir}/last-checkpoint-{num_fold}.bin')\n",
        "\n",
        "            if f_best:\n",
        "                self.save(f'{self.base_dir}/best-checkpoint-{num_fold}.bin')\n",
        "                print('New best checkpoint')\n",
        "\n",
        "            self.epoch += 1\n",
        "\n",
        "    def validation(self, val_loader):\n",
        "        summary_loss = LossMeter()\n",
        "        final_scores = AccMeter()\n",
        "\n",
        "        t = int(time.time())\n",
        "        for step, (images, add_feat, labels) in enumerate(val_loader):\n",
        "            with torch.no_grad():\n",
        "                labels = labels.unsqueeze(1).to(self.device)\n",
        "                images = images.to(self.device)\n",
        "                if add_feat is not None:\n",
        "                    add_feat = add_feat.to(self.device)\n",
        "                batch_size = images.shape[0]\n",
        "\n",
        "                outputs = self.model(images, add_feat)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                final_scores.update(labels, outputs)\n",
        "                summary_loss.update(loss.detach().item(), batch_size)\n",
        "        return summary_loss, final_scores\n",
        "\n",
        "    def train_one_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        summary_loss = LossMeter()\n",
        "        final_scores = AccMeter()\n",
        "\n",
        "        t = int(time.time())\n",
        "        for step, (images, add_feat, labels) in enumerate(train_loader):\n",
        "            labels = labels.unsqueeze(1).to(self.device)\n",
        "            images = images.to(self.device)\n",
        "            if add_feat is not None:\n",
        "                add_feat = add_feat.to(self.device)\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(images, add_feat)\n",
        "\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            final_scores.update(labels.detach(), outputs.detach())\n",
        "            summary_loss.update(loss.detach().item(), batch_size)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return summary_loss, final_scores\n",
        "\n",
        "    def save(self, path):\n",
        "        self.model.eval()\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'best_summary_loss': self.best_summary_loss,\n",
        "            'epoch': self.epoch,\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        checkpoint = torch.load(path)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
        "        self.epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    def log(self, message):\n",
        "        print(message)\n",
        "        with open(self.log_path, 'a+') as logger:\n",
        "            logger.write(f'{message}\\n')\n",
        "\n",
        "def run_inference(test_X, num_fold, test_X_add=None):\n",
        "\n",
        "    test_dataset = ECGDataset(\n",
        "        X = test_X,\n",
        "        X_add = test_X_add,\n",
        "        transforms=get_valid_transforms(),\n",
        "        train=False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=DataLoaderConfig.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=DataLoaderConfig.num_workers\n",
        "    )\n",
        "\n",
        "    checkpoint = torch.load(f'./best-checkpoint-{num_fold}.bin')\n",
        "    net.load_state_dict(checkpoint['model_state_dict'])\n",
        "    net.eval()\n",
        "    print(\"model loaded\")\n",
        "\n",
        "    result = []\n",
        "    for step, (images, add_feat) in enumerate(test_loader):\n",
        "        print(step, end='\\r')\n",
        "        if add_feat is not None:\n",
        "            add_feat = add_feat.to(DEVICE)\n",
        "        y_pred = net(images.to(DEVICE), add_feat).detach().sigmoid().cpu().numpy().ravel()\n",
        "        result.extend(y_pred)\n",
        "    return np.array(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fVnIONlGHxid"
      },
      "cell_type": "code",
      "source": [
        "class DataLoaderConfig:\n",
        "    batch_size = 32\n",
        "    num_workers = 8\n",
        "\n",
        "class TrainConfig:\n",
        "    criterion = nn.BCEWithLogitsLoss\n",
        "    n_epochs = 15\n",
        "    n_splits = 10\n",
        "    lr = 0.001\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
        "    # scheduler = torch.optim.lr_scheduler.StepLR\n",
        "    scheduler_params = dict(\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        verbose=False,\n",
        "        threshold=0.0001,\n",
        "        threshold_mode='abs',\n",
        "        cooldown=0,\n",
        "        min_lr=1e-8,\n",
        "        eps=1e-08\n",
        "    )\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "trusted": true,
        "id": "aDaFaqR3Hxid"
      },
      "cell_type": "code",
      "source": [
        "seed_everything(SEED)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=TrainConfig.n_splits)\n",
        "y_preds = np.zeros(len(df_train), np.float32)\n",
        "y_trues = np.zeros(len(df_train), np.float32)\n",
        "for fold, (train_index, valid_index) in enumerate(skf.split(np.arange(len(df_train)), y=df_train[col_target])):\n",
        "    train_X, train_X_add, train_y, valid_X, valid_X_add, valid_y = ecg_train[train_index], df_train.iloc[train_index][col_features].values, target_train[train_index], ecg_train[valid_index], df_train.iloc[valid_index][col_features].values, target_train[valid_index]\n",
        "    print('-'*30)\n",
        "    print(f'fold: {fold}')\n",
        "    net = Anomaly_Classifier(input_size=12, num_classes=1).to(DEVICE)\n",
        "    run_one_fold(train_X, train_y, valid_X, valid_y, fold, train_X_add=train_X_add, valid_X_add=valid_X_add)\n",
        "\n",
        "    y_pred = run_inference(valid_X, fold, test_X_add=valid_X_add)\n",
        "    y_preds[valid_index] = y_pred\n",
        "    y_trues[valid_index] = valid_y\n",
        "\n",
        "cv = roc_auc_score(y_trues, y_preds)\n",
        "print(f'AUC CV: {cv}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvLvXt5nHxid"
      },
      "cell_type": "markdown",
      "source": [
        "# submit"
      ]
    },
    {
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "Zj1gMNd8Hxid"
      },
      "cell_type": "code",
      "source": [
        "y_preds_test = []\n",
        "for fold in range(TrainConfig.n_splits):\n",
        "    y_pred = run_inference(ecg_test, fold, test_X_add=df_test[col_features].values)\n",
        "    y_preds_test += [y_pred]\n",
        "y_preds_test_mean = np.array(y_preds_test).mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Er7pwjr8Hxie"
      },
      "cell_type": "code",
      "source": [
        "df_sub = pd.read_csv(f'{data_dir}/sample_submission.csv')\n",
        "df_sub['target'] = y_preds_test_mean\n",
        "df_sub.to_csv(\"submission.csv\", index=None)\n",
        "df_sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "notebook29da6a8619",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}