{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitsuo/juntendo-hds/blob/main/ECG_Baseline_210326_%E6%94%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ydata-profiling"
      ],
      "metadata": {
        "id": "LcQb5sxnjtP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "b-8GY9ZF9jmt"
      },
      "cell_type": "code",
      "source": [
        "# ライブラリ読み込み\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os, glob, pickle, time, gc, copy, sys\n",
        "import ydata_profiling as ydp\n",
        "import warnings\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 100) # 表示できる表の列数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6nOTRGal9jmu"
      },
      "cell_type": "markdown",
      "source": [
        "# データ読み込み\n",
        "**ファイル**\n",
        "- `train.csv` - 学習用データ\n",
        "- `test.csv` - テスト用データ\n",
        "- `sample_submission.csv` - 提出ファイルのサンプル, 0 から 1 の間の値の予測結果を target 列に代入して提出してください.\n",
        "- `ecg/` - 心電図データを含むディレクトリ. 各心電図データはnumpy形式で `心電図ID.npy`の名前で保存されています. データの形は(800, 12)です。\n",
        " - 第1次元 - 時間解像度: 100 Hz × 8 秒 = 800 timepoint. 単位: mV\n",
        " - 第2次元 - 12誘導: I, II III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6  \n",
        "\n",
        "**データ内容**\n",
        "- `Id` - 匿名化された心電図ID. 全ての心電図データはそれぞれ異なる患者から測定されています.\n",
        "- `target` - 目的変数. 0=正常, 1=心筋梗塞 (test.csv にはありません)\n",
        "- `age` - 年齢\n",
        "- `sex` - 性別. 0=男性, 1=女性.\n",
        "- `label_type` ラベル作成方法. auto=自動解析, human=医師による診断."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3A1_BmDc-b"
      },
      "source": [
        "# 心電図データのダウンロード\n",
        "今回は 公全国医療AIコンテスト 2021 (https://www.kaggle.com/competitions/ai-medical-contest-2021/)の心電図データをダウンロードします。このデータは心電図から心筋梗塞かどうかを判定するタスクのためのものです。\n",
        "\n",
        "まず **linuxコマンド** を使って 公開されているデータをダウンロードしましょう。下のセルでは `ai-medical-contest-2021` というディレクトリをデフォルトのディレクトリ `/content` の下に作成し、そこに関連データをダウンロードしています。\n",
        "\n",
        "> コマンドとはコンピュータに計算やファイルの操作を行うように指示できるツールです。今回は詳しく触れませんが、colabのようなノートブックではpythonとコマンドを混ぜて使えるのが一つの強みです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTN_9KKyMGIJ"
      },
      "source": [
        "# ! を先頭につけると一時的に適応される。例えばワーキングディレクトリの移動をしてもその後のコマンドには適応されない。\n",
        "# ai-medical-contest-2021 ディレクトリを作成します。\n",
        "!rm -rf /content/ai-medical-contest-2021/\n",
        "!mkdir /content/ai-medical-contest-2021\n",
        "!pwd\n",
        "\n",
        "# % を先頭につけると永続的に適応される。ワーキングディレクトリの移動をしてもその後も適応される。\n",
        "# ai-medical-contest-2021 ディレクトリに移動します。\n",
        "%cd /content/ai-medical-contest-2021\n",
        "!ls\n",
        "\n",
        "# 心電図データのダウンロード\n",
        "!wget http://mitsuo.nishizawa.com/juntendo/ai-medical-contest-2021.zip\n",
        "!unzip ai-medical-contest-2021.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxQWY82dKKok"
      },
      "source": [
        "ダウンロードされたファイルは左のバーのフォルダーマークを押すとみることができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf-MR6g9GLhB"
      },
      "source": [
        "今回はいろんなPythonモジュールを使って、一つの心音データを図示することを目標にしましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iqd8YAEkAmi7"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8Nuz7yuq9jmv"
      },
      "cell_type": "code",
      "source": [
        "# trainファイルを読み込む\n",
        "df_train = pd.read_csv(\"/content/ai-medical-contest-2021/train.csv\")\n",
        "print(\"df_train.shape\", df_train.shape) # シェイプ = (行数, 列数)を表示する\n",
        "df_train.head() # 先頭5行を表示する"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3HXAP91o9jmv"
      },
      "cell_type": "code",
      "source": [
        "# testファイルを読み込む\n",
        "df_test = pd.read_csv(\"/content/ai-medical-contest-2021/test.csv\")\n",
        "print(\"df_test.shape\", df_test.shape) # シェイプ = (行数, 列数)を表示する\n",
        "df_test.head() # 先頭5行を表示する"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7lysB_5F9jmw"
      },
      "cell_type": "code",
      "source": [
        "# submissionファイルを読み込む\n",
        "df_sub = pd.read_csv(\"/content/ai-medical-contest-2021/sample_submission.csv\")\n",
        "print(\"df_sub.shape\", df_sub.shape) # シェイプ = (行数, 列数)を表示する\n",
        "df_sub.head() # 先頭5行を表示する"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Hv0G33I59jmw"
      },
      "cell_type": "code",
      "source": [
        "# ECGデータのpathの列を追加.\n",
        "df_train['path'] = df_train['Id'].apply(lambda x: \"/content/ai-medical-contest-2021/ecg/{}.npy\".format(x))\n",
        "df_test['path'] = df_test['Id'].apply(lambda x: \"/content/ai-medical-contest-2021/ecg/{}.npy\".format(x))\n",
        "print(df_train['path'][0]) # path列の0行目を表示\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3ELS9fik9jmx"
      },
      "cell_type": "code",
      "source": [
        "# trainとtestを連結する\n",
        "df_traintest = pd.concat([df_train, df_test]).reset_index(drop=True) # reset_index: 行のindexをリセットする\n",
        "print(df_traintest.shape)\n",
        "df_traintest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DkTiV8iI9jmx"
      },
      "cell_type": "markdown",
      "source": [
        "# EDA (Explanatory Data Analysis, 探索的データ解析)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FZQbLopz9jmx"
      },
      "cell_type": "code",
      "source": [
        "# ターゲットについて解析\n",
        "col_target = 'target' # ターゲットの列\n",
        "col_index = 'Id' # idの列\n",
        "print(\"rate of positive: {:.6f}\".format(df_train[col_target].mean())) # targetが1である割合"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xYpSKvCV9jmz"
      },
      "cell_type": "code",
      "source": [
        "# 各列の基本情報を表示\n",
        "# 解析対象はtrain+test\n",
        "# 列名, 型, nanの数, uniqueな値の数, 実際の値の一部, を表示する\n",
        "df_tmp = df_traintest  # 解析するDataFrameを指定\n",
        "for i, col in enumerate(df_tmp.columns): # 各列(column)について\n",
        "    col_name = col + \" \" * (22 - len(col)) # カラム名, 見た目上の整形のためにスペースを加える\n",
        "    type_name = \"{}\".format(df_tmp[col].dtype) # 型名\n",
        "    type_name = type_name + \" \" * (8 - len(type_name)) # 見た目上の整形のためにスペースを加える\n",
        "    num_unique = len(df_tmp[col].unique()) # ユニークな値の数\n",
        "    num_nan = pd.isna(df_tmp[col]).sum() # nanの数\n",
        "    col_head = \"{}\".format(df_tmp[col].unique()[:5].tolist())[:40] # 実際の値の一部\n",
        "    print(\"{:4d}: {} dtype: {} unique: {:8d}, nan: {:6d}, 実際の値: {}\".format(\n",
        "        i, col_name, type_name, len(df_tmp[col].unique()), num_nan, col_head)) # 表示する"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "iPX-4nOH9jmz"
      },
      "cell_type": "code",
      "source": [
        "# ydata_profiling でデータの解析を行う\n",
        "# 解析対象はtrain+test\n",
        "ydp.ProfileReport(df_traintest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Qv8phqs99jm0"
      },
      "cell_type": "code",
      "source": [
        "# 数値変数についてヒストグラムを表示する\n",
        "# 解析対象はtrain+test\n",
        "from scipy.stats import norm\n",
        "cols_num = ['age'] # 数値変数の列名のリスト, このデータの場合 age のみ\n",
        "fig = plt.figure(figsize=(20, int(4*int(np.ceil(len(cols_num)/4)))))\n",
        "for i, col in enumerate(cols_num[:]):\n",
        "    ax = fig.add_subplot(int(np.ceil(len(cols_num)/4)),4,i+1)\n",
        "    sns.distplot(\n",
        "        df_traintest[col], # 表示するデータ\n",
        "        bins=20, # ヒストグラムのビンの数\n",
        "        color='black', label='data',\n",
        "        kde_kws={'label': 'kde','color':'r'},\n",
        "#         fit=norm,\n",
        "#         fit_kws={'label': 'norm','color':'red'},\n",
        "        rug=False\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8XWkilEB9jm1"
      },
      "cell_type": "code",
      "source": [
        "# カテゴリ変数について棒グラフを表示する\n",
        "# 解析対象はtrain+test\n",
        "cols_cat = ['sex', 'label_type'] # 解析する列名\n",
        "for i, col in enumerate(cols_cat):\n",
        "    g = sns.catplot(\n",
        "        x=col,\n",
        "        kind=\"count\",\n",
        "        data=df_traintest, # 解析するDataFrame\n",
        "        height=5,\n",
        "        palette=\"muted\"\n",
        "    )\n",
        "    g.fig.set_figwidth(16)\n",
        "    g.fig.set_figheight(2)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Si9FsO6M9jm1"
      },
      "cell_type": "code",
      "source": [
        "# ターゲットの値ごとに数値変数のバイオリンプロットを表示\n",
        "# target=0の場合のageの分布、target=1の場合のageの分布を表示する\n",
        "# 2つの分布が違うという事はその変数がターゲットに強い影響を与えていることを示唆する\n",
        "# 解析対象はtrainのみ\n",
        "fig = plt.figure(figsize=(20, int(4*int(np.ceil(len(cols_num)/4)))))\n",
        "for i, col in enumerate(cols_num):\n",
        "    ax = fig.add_subplot(int(np.ceil(len(cols_num)/4)),4,i+1)\n",
        "    sns.violinplot(\n",
        "        x=col_target,\n",
        "        y=col,\n",
        "        data=df_train,\n",
        "#         scale='count', # 消すと正規化する\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Hf-dGESq9jm1"
      },
      "cell_type": "code",
      "source": [
        "# 数値変数について logistic regression plot\n",
        "# age を説明変数として target をアウトカムとするロジスティック回帰を行ってそれを表示\n",
        "# 解析対象はtrainのみ\n",
        "fig = plt.figure(figsize=(20, int(4*int(np.ceil(len(cols_num)/4)))))\n",
        "for i, col in enumerate(cols_num[:]):\n",
        "    ax = fig.add_subplot(int(np.ceil(len(cols_num)/4)),4,i+1)\n",
        "    sns.regplot(x=col,\n",
        "                y=col_target,\n",
        "                data=df_train,\n",
        "                logistic=True,\n",
        "                scatter_kws={'s': 10, 'alpha':0.3,'color':'m'},\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-tdMyO4q9jm2"
      },
      "cell_type": "code",
      "source": [
        "# カテゴリ変数について積み上げ棒グラフを表示\n",
        "# 解析対象はtrainのみ\n",
        "def stack_bar_plot(df_tmp, col, col_target, ax=None):\n",
        "    df_tmp[col][pd.isna(df_tmp[col])] = 'nan'\n",
        "    target_value = df_tmp[col].unique()\n",
        "    df_agg = df_tmp[df_tmp[col_target].duplicated()==False][[col_target]].sort_values(col_target).reset_index(drop=True)\n",
        "    for value in target_value:\n",
        "        col_value = \"{}\".format(value)\n",
        "        df_agg_tmp = df_tmp[df_tmp[col]==value].groupby(col_target)[col].agg(len).reset_index()\n",
        "        df_agg_tmp = df_agg_tmp.sort_values(col_target).reset_index(drop=True)\n",
        "        df_agg_tmp.columns = [col_target, col_value]\n",
        "        df_agg = pd.merge(df_agg, df_agg_tmp, on=col_target, how='left')\n",
        "    df_agg = df_agg.fillna(0)\n",
        "    col_new = \"{}/{}\".format(col, col_target)\n",
        "    df_agg.columns = [col_new] + df_agg.columns[1:].values.tolist()\n",
        "    df_agg = df_agg.set_index(col_new)\n",
        "    df_agg.iloc[:] = df_agg.values / df_agg.values.sum(axis=1)[:,np.newaxis]\n",
        "    ax1 = df_agg.plot.bar(stacked=True, ax=ax)\n",
        "    ax1.legend(title=col)\n",
        "\n",
        "\n",
        "cols_cat = ['sex', 'label_type'] # 解析する列名\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "for i, col in enumerate(cols_cat):\n",
        "    stack_bar_plot(df_train, col, col_target, ax=axes[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "l-XixaCN9jm2"
      },
      "cell_type": "code",
      "source": [
        "# train, testそれぞれのlabel_typeの分布を確認する\n",
        "g = sns.catplot(\n",
        "        x='label_type',\n",
        "        kind=\"count\",\n",
        "        data=df_train, # 解析するDataFrame\n",
        "        height=5,\n",
        "        palette=\"muted\"\n",
        ")\n",
        "g.fig.set_figwidth(16)\n",
        "g.fig.set_figheight(2)\n",
        "plt.title(\"train data\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "g = sns.catplot(\n",
        "        x='label_type',\n",
        "        kind=\"count\",\n",
        "        data=df_test, # 解析するDataFrame\n",
        "        height=5,\n",
        "        palette=\"muted\"\n",
        ")\n",
        "g.fig.set_figwidth(16)\n",
        "g.fig.set_figheight(2)\n",
        "plt.title(\"test data\", fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hDSmJ1y49jm3"
      },
      "cell_type": "code",
      "source": [
        "# 心電図波形を表示する\n",
        "index = 10 # 表示する行を指定\n",
        "path_tmp = df_train['path'][index] # 表示する心電図データのpath\n",
        "ecg_tmp = np.load(path_tmp) # 心電図データのよみこみ\n",
        "print(\"i: {}, id: {}\".format(index, df_train[col_index][index]))\n",
        "print(\"ECG shape: {}\".format(ecg_tmp.shape))\n",
        "print(\"target: {}\".format(df_train[col_target][index]))\n",
        "lead_list = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6'] # 各誘導の名称\n",
        "plt.figure(figsize=(20,16))\n",
        "for i in range(12): # 各誘導を可視化\n",
        "    if i<6:\n",
        "        plt.subplot(6,2,i*2+1)\n",
        "    else:\n",
        "        plt.subplot(6,2,i*2-10)\n",
        "    plt.plot(ecg_tmp[:,i])\n",
        "    plt.xticks(np.arange(0, 801, step=100))\n",
        "    plt.ylabel(lead_list[i]+\"  \", rotation=0, fontsize=16)\n",
        "    plt.minorticks_on()\n",
        "    plt.ylim(ecg_tmp.min(),ecg_tmp.max())\n",
        "    plt.grid(which=\"major\", color=\"black\", alpha=0.5)\n",
        "    plt.grid(which=\"minor\", color=\"gray\", linestyle=\":\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xdgjHJ6c9jm3"
      },
      "cell_type": "markdown",
      "source": [
        "# 前処理"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zZB4GjjQ9jm3"
      },
      "cell_type": "code",
      "source": [
        "# カテゴリ変数をラベルエンコーディングする (数値に置き換える).\n",
        "df_traintest['sex'] = df_traintest['sex'].replace('female', 0) # femaleに0を代入\n",
        "df_traintest['sex'] = df_traintest['sex'].replace('male', 1) # maleに1を代入\n",
        "df_traintest['sex'] = df_traintest['sex'].astype(int) # 型を整数に変換\n",
        "\n",
        "df_traintest['label_type'] = df_traintest['label_type'].replace('human', 0) # humanに0を代入\n",
        "df_traintest['label_type'] = df_traintest['label_type'].replace('auto', 1) # autoに1を代入\n",
        "df_traintest['label_type'] = df_traintest['label_type'].astype(int) # 型を整数に変換\n",
        "df_traintest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "UL4HrwKV9jm3"
      },
      "cell_type": "code",
      "source": [
        "# train と test を再度切り分ける\n",
        "df_train = df_traintest.iloc[:len(df_train)]\n",
        "df_test = df_traintest.iloc[len(df_train):].reset_index(drop=True)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XvdY-6WP9jm4"
      },
      "cell_type": "code",
      "source": [
        "# 全てのECGデータを読み込む\n",
        "ecg_train = np.zeros([len(df_train), 800, 12], np.float32) # trainの心電図データの代入先. shape=(データ数, 時間方向, 12誘導)\n",
        "for i in range(len(df_train)): # 全てのtrain dataについて\n",
        "    path_tmp = df_train['path'][i] # i行目の心電図データのpath\n",
        "    ecg_tmp = np.load(path_tmp) # i行目の心電図データ\n",
        "    ecg_train[i] = ecg_tmp # 読み込んだ心電図データをecg_trainのi行目に代入\n",
        "\n",
        "ecg_test = np.zeros([len(df_test), 800, 12], np.float32) # testの心電図データの代入先. shape=(データ数, 時間方向, 12誘導)\n",
        "for i in range(len(df_test)): # 全てのtest dataについて\n",
        "    path_tmp = df_test['path'][i] # i行目の心電図データのpath\n",
        "    ecg_tmp = np.load(path_tmp) # i行目の心電図データ\n",
        "    ecg_test[i] = ecg_tmp # 読み込んだ心電図データをecg_trainのi行目に代入\n",
        "print(\"ecg_train.shape: {}\".format(ecg_train.shape))\n",
        "print(\"ecg_test.shape: {}\".format(ecg_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "BktfUG_T9jm4"
      },
      "cell_type": "code",
      "source": [
        "# target情報をnumpy形式に変換\n",
        "target_train = df_train[col_target].values.astype(int) # pandas.Seriesからnp.ndarrayへ変換\n",
        "print(\"target_train.shape: {}\".format(target_train.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "52jbrWrt9jm4"
      },
      "cell_type": "markdown",
      "source": [
        "# ベースラインモデルを作成\n",
        "### クロスバリデーション\n",
        "![CV](https://jp.mathworks.com/discovery/cross-validation/_jcr_content/mainParsys/image.adapt.480.medium.jpg/1611249616013.jpg)\n",
        "https://jp.mathworks.com/discovery/cross-validation.html"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tI6w8nH59jm4"
      },
      "cell_type": "code",
      "source": [
        "# クロスバリデーションを行うためにデータを5分割する\n",
        "# 4つを学習に用い、1つを検証に要する。これを5回繰り返す。\n",
        "folds = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(\n",
        "    np.arange(len(df_train)),\n",
        "    y=df_train[col_target]) # 各foldターゲットのラベルの分布がそろうようにする = stratified K fold\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zcv2oeWZ9jm5"
      },
      "cell_type": "code",
      "source": [
        "# fold 0の学習データと検証データの分割\n",
        "fold = 0 # fold 0 についての学習を行う\n",
        "\n",
        "# このfoldにおける学習データと検証データの切り分け\n",
        "X_train = ecg_train[folds[fold][0]] # 学習データの入力データを抽出\n",
        "y_train = target_train[folds[fold][0]] # 学習データの正解データを抽出\n",
        "X_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\n",
        "y_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\n",
        "print(\"X_train.shape: {}, X_valid.shape: {}\".format(X_train.shape, X_valid.shape))\n",
        "print(\"y_train.shape: {}, y_valid.shape: {}\".format(y_train.shape, y_valid.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "BuAQ_UcC9jm5"
      },
      "cell_type": "code",
      "source": [
        "# modelにdataを流すためのdatasetを構築する\n",
        "BATCH_SIZE = 64 # ミニバッチに含めるデータの数\n",
        "def augment_fn(X, y):\n",
        "    \"\"\"\n",
        "    augmentation (データ水増し)を設定する\n",
        "    \"\"\"\n",
        "    X_new = tf.image.random_crop(X, (700,12)) # 時間方向に800 timepointからrandomに700 timepointを切り出す\n",
        "    return (X_new, y)\n",
        "\n",
        "# train dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(( # np\n",
        "    X_train, # 入力データ\n",
        "    y_train, # 正解データ\n",
        "))\n",
        "train_dataset = train_dataset.shuffle(len(train_dataset), reshuffle_each_iteration=True) # 学習中にデータをシャッフルすることを指定する\n",
        "train_dataset = train_dataset.map(augment_fn, num_parallel_calls=AUTOTUNE) # augmentationの適用\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する\n",
        "\n",
        "# valid dataset\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    X_valid, # 入力データ\n",
        "    y_valid, # 正解データ\n",
        "))\n",
        "valid_dataset = valid_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する (シャッフルはしない)\n",
        "\n",
        "# test dataset\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    ecg_test, # 入力データ\n",
        "    np.zeros(len(ecg_test)), # 正解データ (testに正解データないためダミーデータ)\n",
        "))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する (シャッフルはしない)\n",
        "\n",
        "\n",
        "# datasetの読み込みテスト\n",
        "ecg_batch, target_batch = next(iter(train_dataset)) # 試しにミニバッチを読み込む\n",
        "print(\"train ecg_batch.shape: {}\".format(ecg_batch.shape))\n",
        "print(\"train target_batch.shape: {}\".format(target_batch.shape))\n",
        "ecg_batch, target_batch = next(iter(valid_dataset)) # 試しにミニバッチを読み込む\n",
        "print(\"valid ecg_batch.shape: {}\".format(ecg_batch.shape))\n",
        "print(\"valid target_batch.shape: {}\".format(target_batch.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "abr85xF59jm5"
      },
      "cell_type": "markdown",
      "source": [
        "### Convolutional neural networks\n",
        "![CNN](https://cdn-ak.f.st-hatena.com/images/fotolife/a/acro-engineer/20180318/20180318155516.png)\n",
        "(LeCun et al. 1998)  "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rgCf17_X9jm5"
      },
      "cell_type": "code",
      "source": [
        "# deep learning modelの作成\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "def get_model(input_shape=(800, 12)):\n",
        "    model = tf.keras.models.Sequential([ # レイヤーのリストからモデルを構築する\n",
        "        tf.keras.Input(shape=input_shape), # 入力の形状の指定. shape=(時間軸, 12誘導)\n",
        "        # block1\n",
        "        tf.keras.layers.Conv1D(64, 7), # 時間方向の1次元畳み込みレイヤー. 32=出力チャネル数, 7=カーネルサイズ\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "        tf.keras.layers.MaxPool1D(2),\n",
        "        # block2\n",
        "        tf.keras.layers.Conv1D(128, 3, strides=2),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "        # block3\n",
        "        tf.keras.layers.Conv1D(256, 3, strides=2),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "        # pooling\n",
        "        tf.keras.layers.GlobalAveragePooling1D(), # 時間方向のglobal pooling\n",
        "        # 最終レイヤー\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fCOC5VYJ9jm5"
      },
      "cell_type": "code",
      "source": [
        "# モデルのコンパイル (学習条件の設定)\n",
        "model = get_model(input_shape=(None, 12)) # 時間軸の入力長さNone=可変にして再度model構築\n",
        "model.compile(optimizer='adam', # オプティマイザーにAdamを指定\n",
        "              loss='binary_crossentropy', # 損失関数にbinary crossentropyを指定\n",
        "              metrics=['AUC'], # 評価関数にAUCを指定\n",
        "             )\n",
        "# モデルの保存方法の指定\n",
        "checkpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath, # 保存path\n",
        "    save_weights_only=True, # 重みのみを保存\n",
        "    monitor='val_auc', # validataionのAUCの値に基づいて重みを保存する\n",
        "    mode='max', # validataionのAUCが最大となった時重みを保存する\n",
        "    save_best_only=True # AUCが改善したときのみ保存する\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8lm3mybm9jm5"
      },
      "cell_type": "code",
      "source": [
        "# モデルの学習\n",
        "model.fit(\n",
        "    train_dataset, # 学習に用いるdataset\n",
        "    validation_data=valid_dataset, # 検証に用いるdataset\n",
        "    callbacks=[model_checkpoint_callback], # モデル保存方法の指定\n",
        "    epochs=16, # epoch数 (1epoch=すべての画像を1回ずつ学習に利用する)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QY9NWxKE9jm6"
      },
      "cell_type": "code",
      "source": [
        "# fold1-4についても学習を行う\n",
        "# for loopの中身は上記のfold 0での処理と同様です\n",
        "for fold in range(1,5):\n",
        "    print(\"fold: {}\".format(fold))\n",
        "    X_train = ecg_train[folds[fold][0]] # 学習データの入力データを抽出\n",
        "    y_train = target_train[folds[fold][0]] # 学習データの正解データを抽出\n",
        "    X_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\n",
        "    y_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\n",
        "    print(\"len train: {}. len valid: {}\".format(len(X_train), len(X_valid)))\n",
        "\n",
        "    # train dataset\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        X_train, # 入力データ\n",
        "        y_train, # 正解データ\n",
        "    ))\n",
        "    train_dataset = train_dataset.shuffle(len(train_dataset), reshuffle_each_iteration=True) # 学習中にデータをシャッフルすることを指定する\n",
        "    train_dataset = train_dataset.map(augment_fn, num_parallel_calls=AUTOTUNE) # augmentationの適用\n",
        "    train_dataset = train_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する\n",
        "\n",
        "    # valid dataset\n",
        "    valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        X_valid, # 入力データ\n",
        "        y_valid, # 正解データ\n",
        "    ))\n",
        "    valid_dataset = valid_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する (シャッフルはしない)\n",
        "\n",
        "    # model構築\n",
        "    model = get_model(input_shape=(None, 12))\n",
        "    # モデルのコンパイル (学習条件の設定)\n",
        "    model.compile(optimizer='adam', # オプティマイザーにAdamを指定\n",
        "                  loss='binary_crossentropy', # 損失関数にbinary crossentropyを指定\n",
        "                  metrics=['AUC'], # 評価関数にAUCを指定\n",
        "                 )\n",
        "    # モデルの保存方法の指定\n",
        "    checkpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath, # 保存path\n",
        "        save_weights_only=True, # 重みのみを保存\n",
        "        monitor='val_auc', # validataionのAUCの値に基づいて重みを保存する\n",
        "        mode='max', # validataionのAUCが最大となった時重みを保存する\n",
        "        save_best_only=True # AUCが改善したときのみ保存する\n",
        "    )\n",
        "    # モデルの学習\n",
        "    model.fit(\n",
        "        train_dataset, # 学習に用いるdataset\n",
        "        validation_data=valid_dataset, # 検証に用いるdataset\n",
        "        callbacks=[model_checkpoint_callback], # モデル保存方法の指定\n",
        "        epochs=16, # epoch数 (1epoch=すべての画像を1回ずつ学習に利用する)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dvSnSR129jm6"
      },
      "cell_type": "markdown",
      "source": [
        "# 学習したモデルを評価する"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xagWRlCq9jm6"
      },
      "cell_type": "code",
      "source": [
        "# クロスバリデーションのAUCを計算する\n",
        "preds_valid = np.zeros(len(df_train), np.float32) # 予測結果の代入先\n",
        "for fold in range(5): # 各foldについて\n",
        "    print(\"fold: {}\".format(fold))\n",
        "    # valid dataset\n",
        "    X_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\n",
        "    y_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\n",
        "    valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        X_valid, # 入力データ\n",
        "        y_valid, # 正解データ\n",
        "    ))\n",
        "    valid_dataset = valid_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する (シャッフルはしない)\n",
        "\n",
        "    # 予測\n",
        "    checkpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\n",
        "    model.load_weights(checkpoint_filepath) # 最もvalid AUCが高かったエポックの重みを読み込む\n",
        "    pred_valid = model.predict(valid_dataset) # 予測の実行\n",
        "    preds_valid[folds[fold][1]] = pred_valid[:,0] # 予測結果の代入\n",
        "\n",
        "valid_auc = metrics.roc_auc_score(df_train[col_target], preds_valid)\n",
        "print(\"CV: {:.6f}\".format(valid_auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "G-tNLiHH9jm6"
      },
      "cell_type": "code",
      "source": [
        "# 予測結果のヒストグラムを表示\n",
        "idx_nega = df_train[col_target].astype(int)==0\n",
        "idx_posi = df_train[col_target].astype(int)==1\n",
        "plt.figure(figsize=(16,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(preds_valid[idx_nega], bins=np.arange(101)/100, alpha=0.3, label='negative')\n",
        "plt.hist(preds_valid[idx_posi], bins=np.arange(101)/100, alpha=0.3, label='positive')\n",
        "plt.legend()\n",
        "plt.xlabel(\"predict\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "oEOWRVSD9jm7"
      },
      "cell_type": "code",
      "source": [
        "# labelがpositiveであり予測がpositiveだった心電図を表示\n",
        "df_tmp = copy.deepcopy(df_train)\n",
        "df_tmp['pred'] = preds_valid\n",
        "df_positive = df_tmp[df_tmp[col_target]==1]\n",
        "df_positive = df_positive.sort_values('pred', ascending=False).reset_index(drop=True)\n",
        "index = 0 # 表示する行を指定\n",
        "path_tmp = df_positive['path'][index] # 表示する心電図データのpath\n",
        "ecg_tmp = np.load(path_tmp) # 心電図データのよみこみ\n",
        "print(\"easiest positive case\")\n",
        "print(\"id: {}\".format(df_positive[col_index][index]))\n",
        "print(\"predict: {:.6f}\".format(df_positive['pred'][index]))\n",
        "lead_list = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6'] # 各誘導の名称\n",
        "plt.figure(figsize=(20,16))\n",
        "for i in range(12): # 各誘導を可視化\n",
        "    if i<6:\n",
        "        plt.subplot(6,2,i*2+1)\n",
        "    else:\n",
        "        plt.subplot(6,2,i*2-10)\n",
        "    plt.plot(ecg_tmp[:,i])\n",
        "    plt.xticks(np.arange(0, 801, step=100))\n",
        "    plt.ylabel(lead_list[i]+\"  \", rotation=0, fontsize=16)\n",
        "    plt.minorticks_on()\n",
        "    plt.ylim(ecg_tmp.min(),ecg_tmp.max())\n",
        "    plt.grid(which=\"major\", color=\"black\", alpha=0.5)\n",
        "    plt.grid(which=\"minor\", color=\"gray\", linestyle=\":\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ohZ-E2lg9jm7"
      },
      "cell_type": "code",
      "source": [
        "# labelがpositiveであるのに予測がnegativeだった心電図を表示\n",
        "df_tmp = copy.deepcopy(df_train)\n",
        "df_tmp['pred'] = preds_valid\n",
        "df_positive = df_tmp[df_tmp[col_target]==1]\n",
        "df_positive = df_positive.sort_values('pred').reset_index(drop=True)\n",
        "index = 0 # 表示する行を指定\n",
        "path_tmp = df_positive['path'][index] # 表示する心電図データのpath\n",
        "ecg_tmp = np.load(path_tmp) # 心電図データのよみこみ\n",
        "print(\"most difficult positive case\")\n",
        "print(\"id: {}\".format(df_positive[col_index][index]))\n",
        "print(\"predict: {:.6f}\".format(df_positive['pred'][index]))\n",
        "lead_list = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6'] # 各誘導の名称\n",
        "plt.figure(figsize=(20,16))\n",
        "for i in range(12): # 各誘導を可視化\n",
        "    if i<6:\n",
        "        plt.subplot(6,2,i*2+1)\n",
        "    else:\n",
        "        plt.subplot(6,2,i*2-10)\n",
        "    plt.plot(ecg_tmp[:,i])\n",
        "    plt.xticks(np.arange(0, 801, step=100))\n",
        "    plt.ylabel(lead_list[i]+\"  \", rotation=0, fontsize=16)\n",
        "    plt.minorticks_on()\n",
        "    plt.ylim(ecg_tmp.min(),ecg_tmp.max())\n",
        "    plt.grid(which=\"major\", color=\"black\", alpha=0.5)\n",
        "    plt.grid(which=\"minor\", color=\"gray\", linestyle=\":\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fqNbP4yr9jm7"
      },
      "cell_type": "code",
      "source": [
        "# labelがnegativeであり予測がnegativeだった心電図を表示\n",
        "df_tmp = copy.deepcopy(df_train)\n",
        "df_tmp['pred'] = preds_valid\n",
        "df_negative = df_tmp[df_tmp[col_target]==0]\n",
        "df_negative = df_negative.sort_values('pred').reset_index(drop=True)\n",
        "index = 0 # 表示する行を指定\n",
        "path_tmp = df_negative['path'][index] # 表示する心電図データのpath\n",
        "ecg_tmp = np.load(path_tmp) # 心電図データのよみこみ\n",
        "print(\"easiest negative case\")\n",
        "print(\"id: {}\".format(df_negative[col_index][index]))\n",
        "print(\"predict: {:.6f}\".format(df_negative['pred'][index]))\n",
        "lead_list = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6'] # 各誘導の名称\n",
        "plt.figure(figsize=(20,16))\n",
        "for i in range(12): # 各誘導を可視化\n",
        "    if i<6:\n",
        "        plt.subplot(6,2,i*2+1)\n",
        "    else:\n",
        "        plt.subplot(6,2,i*2-10)\n",
        "    plt.plot(ecg_tmp[:,i])\n",
        "    plt.xticks(np.arange(0, 801, step=100))\n",
        "    plt.ylabel(lead_list[i]+\"  \", rotation=0, fontsize=16)\n",
        "    plt.minorticks_on()\n",
        "    plt.ylim(ecg_tmp.min(),ecg_tmp.max())\n",
        "    plt.grid(which=\"major\", color=\"black\", alpha=0.5)\n",
        "    plt.grid(which=\"minor\", color=\"gray\", linestyle=\":\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "m4nu5Z259jm7"
      },
      "cell_type": "code",
      "source": [
        "# labelがnegativeであるのに予測がpositiveだった心電図を表示\n",
        "df_tmp = copy.deepcopy(df_train)\n",
        "df_tmp['pred'] = preds_valid\n",
        "df_negative = df_tmp[df_tmp[col_target]==0]\n",
        "df_negative = df_negative.sort_values('pred', ascending=False).reset_index(drop=True)\n",
        "index = 0 # 表示する行を指定\n",
        "path_tmp = df_negative['path'][index] # 表示する心電図データのpath\n",
        "ecg_tmp = np.load(path_tmp) # 心電図データのよみこみ\n",
        "print(\"most difficult negative case\")\n",
        "print(\"id: {}\".format(df_negative[col_index][index]))\n",
        "print(\"predict: {:.6f}\".format(df_negative['pred'][index]))\n",
        "lead_list = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6'] # 各誘導の名称\n",
        "plt.figure(figsize=(20,16))\n",
        "for i in range(12): # 各誘導を可視化\n",
        "    if i<6:\n",
        "        plt.subplot(6,2,i*2+1)\n",
        "    else:\n",
        "        plt.subplot(6,2,i*2-10)\n",
        "    plt.plot(ecg_tmp[:,i])\n",
        "    plt.xticks(np.arange(0, 801, step=100))\n",
        "    plt.ylabel(lead_list[i]+\"  \", rotation=0, fontsize=16)\n",
        "    plt.minorticks_on()\n",
        "    plt.ylim(ecg_tmp.min(),ecg_tmp.max())\n",
        "    plt.grid(which=\"major\", color=\"black\", alpha=0.5)\n",
        "    plt.grid(which=\"minor\", color=\"gray\", linestyle=\":\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3hAfPbyr9jm7"
      },
      "cell_type": "markdown",
      "source": [
        "# 提出ファイルの作成"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "S73T3P6S9jm8"
      },
      "cell_type": "code",
      "source": [
        "# test dataに対する予測\n",
        "preds_test = np.zeros([5, len(df_test)], np.float32) # 予測結果の代入先\n",
        "for fold in range(5): # 各foldについて\n",
        "    print(\"fold: {}\".format(fold))\n",
        "    # valid dataset\n",
        "    X_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\n",
        "    y_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\n",
        "    valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        X_valid, # 入力データ\n",
        "        y_valid, # 正解データ\n",
        "    ))\n",
        "    valid_dataset = valid_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する (シャッフルはしない)\n",
        "\n",
        "    # 予測\n",
        "    checkpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\n",
        "    model.load_weights(checkpoint_filepath) # 最もvalid AUCが高かったエポックの重みを読み込む\n",
        "    pred_test = model.predict(test_dataset) # 予測の実行\n",
        "    preds_test[fold] = pred_test[:,0] # 予測結果の代入\n",
        "print(\"preds_test.shape: {}\".format(preds_test.shape))\n",
        "print(preds_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8SHwF3749jm8"
      },
      "cell_type": "code",
      "source": [
        "### submitファイルを作成\n",
        "preds_test_mean = preds_test.mean(axis=0) # 各foldのmodelの予測の平均値を最終的な予測結果として採用する\n",
        "print(\"preds_test_mean.shape: {}\".format(preds_test_mean.shape))\n",
        "df_sub[col_target] = preds_test.mean(axis=0) # 推定結果を代入\n",
        "df_sub.to_csv(\"submission.csv\", index=None) # submitファイルを保存\n",
        "df_sub.head() # 最初の5行を表示"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nHzX7VVR9jm8"
      },
      "cell_type": "markdown",
      "source": [
        "# 次にすること\n",
        "- model をさらに深くする\n",
        "- augmentation を加える (noise, amplify, etc)\n",
        "- ハイパーパラメータ･チューニング (epoch を増やす, learning rate を減らす, etc)\n",
        "- メタデータを活用する (sex, age, label_type)\n",
        "- kaggle の解法を参考にする (e.g. https://www.kaggle.com/shayanfazeli/heartbeat)\n",
        "\n",
        "### 参考リンク\n",
        "- [心電図データ×機械学習まとめ](https://medium.com/micin-developers/%E5%BF%83%E9%9B%BB%E5%9B%B3%E3%83%87%E3%83%BC%E3%82%BF-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%BE%E3%81%A8%E3%82%81-ed702a0008d4)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "a7cy2pgW9jm8"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}